# Day 01 — Soft Robotics at MIT CSAIL

Today I dove into an article titled ["Forget Humanoids. At MIT, Worms and Turtles Are Inspiring a New Generation of Robots"](https://www.wsj.com/articles/forget-humanoids-at-mit-worms-and-turtles-are-inspiring-a-new-generation-of-robots-3abf7d5f), published by the Wall Street Journal. It sheds light on a fascinating shift happening in the field of robotics at MIT’s CSAIL lab, where researchers are moving away from the humanoid obsession and embracing a more nature-inspired, problem-oriented philosophy. And honestly, the direction feels not only refreshing, but also more grounded in real-world needs.

The article features the work of Daniela Rus, who’s leading efforts to design robots that look and behave more like animals — turtles, worms, and octopuses — than like people. The idea is simple but profound: most tasks that we expect robots to perform don't actually require them to look like us. For example, a robot designed to inspect underwater pipelines doesn’t need arms or legs; it needs to swim efficiently, navigate complex terrain, and be durable enough to handle pressure and salinity. So instead of mimicking the complexity of the human musculoskeletal system, these researchers are building robots with soft silicone flippers, like sea turtles, that can move gently through water and interact with fragile ecosystems without causing harm.

This philosophy — form following function — reminds me of something I once heard in a design lecture: “You don’t give a fish wheels just because your car has them.” A lot of robotic design, especially in the startup world, tries to make robots look "cool" or "human-like" because it's visually impressive. But MIT’s approach flips that idea on its head. They’re asking: What does the robot actually *need* to do, and what’s the most efficient biological analogy for that?

One of the standout technologies mentioned is something called a **liquid neural network**, inspired by the nervous system of the C. elegans worm, which has only 302 neurons. That’s astonishing, considering how adaptively this tiny organism behaves. Researchers are using this biological insight to create AI models that are dynamic, time-sensitive, and continuously learning — unlike traditional deep learning systems that require static training and struggle to adapt in real-time. The applications for these networks are broad: imagine drones that can react to changing wind patterns mid-flight, or rescue robots navigating unstable disaster zones by adapting their movement on the fly. These are not distant, sci-fi use cases — these are real research directions grounded in biological precedent.

There was also mention of a robot designed for **non-invasive surgeries** — essentially a soft, flexible, sausage-like mechanism that can be inserted into the human body without needing to cut open large areas. It’s guided using magnetics and visual inputs, capable of navigating through the body with minimal intrusion. Think about how that could change the experience of surgery: shorter recovery times, less risk of infection, and more precise internal tasks. It reminded me of how pill-sized cameras are now used for internal diagnostics, but this goes a step further — not just seeing but doing.

Another innovation that caught my attention was an experimental system MIT is working on called **“text-to-robot.”** The idea is to input a natural language prompt like “a robot that can crawl under a table and pick up small objects,” and the AI system will propose a structural and functional design for a robot that meets those requirements. That’s absolutely mind-blowing. It takes us into the realm of no-code robotics — democratizing access to robot creation and reducing the barrier to entry. I could see a future where researchers, educators, even hobbyists can sketch robotic ideas the way we prototype apps with low-code tools today.

But beyond the technologies themselves, what really struck me was the mindset shift. Instead of chasing the dream of human-like androids, MIT’s team is working backward from **real-world problems** — and drawing inspiration from millions of years of evolutionary problem-solving. Nature is the ultimate engineer, and organisms like turtles and worms have been solving survival problems long before we learned how to solder circuits. It’s humbling to see how biology, AI, and robotics intersect here not as gimmickry, but as serious research.

If anything, this article reminded me that sometimes the most “advanced” solutions don’t look futuristic — they look familiar. They’re soft, slow, adaptive, and precise. And they challenge the notion that more complexity equals more intelligence. In the case of liquid neural networks, *less* may actually be more.

Reading this piece was a powerful reminder that engineering isn’t about building the flashiest thing — it’s about building the *right* thing, for the right task, in the right way. And often, the best designs don’t come from our own imaginations, but from careful observation of how life itself works.

This is exactly the kind of robotics I want to understand better — the kind that works with the world instead of against it.

